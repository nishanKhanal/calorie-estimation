---
title: "Multiple Regression Project"
author: "Kabin Devkota, Nishan Khanal and Udita Bista"
date: "4/15/2025"
output: word_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
This analysis focuses on a data set from Kaggle that describes the nutrition facts for McDonald's Menu. This dataset provides a nutrition analysis of every menu item on the US McDonald's menu, including breakfast, beef burgers, chicken and fish sandwiches, fries, salads, soda, coffee and tea, milkshakes, and desserts.
The data for this analysis consists of response variable $y = Calories$ and following explanatory variables:
 
$Category$ 

$item$

$Serving Size$

$Total Fat$

$Total Fat (\% Daily Value)$

$Saturated Fat$

$Saturated Fat (\% Daily Value)$

$Trans Fat$

$Cholesterol$

$Cholesterol (\% Daily Value)$

$Sodium$

$Sodium (\% Daily Value)$

$Carbohydrates$

$Carbohydrates (\% Daily Value)$

$Dietary Fiber$

$Dietary Fiber (\% Daily Value)$

$Sugars$

$Protein$

$Vitamin A (\% Daily Value)$

$Vitamin C (\% Daily Value)$

$Calcium (\% Daily Value)$

$Iron (\% Daily Value)$


```{r}

# library(ggplot2)
library(lmtest, pos=4)
library(corrplot)

# reading the data from the csv file which has some values containing comma enclosed by double quotes
mcdonalds = read.csv("Mcdonalds_menu.csv",header=TRUE,quote="\"",sep=",")
head(mcdonalds)

# getting the column names
colnames(mcdonalds)

# cleaning column names: replace spaces, %, parentheses, etc.
colnames(mcdonalds) <- make.names(colnames(mcdonalds))
```
Out of these 22 explanatory variables, some of the variables like "category", "Total.Fat....Daily.Value.", "Cholesterol....Daily.Value.", etc are either redundant because there already exist other columns that account for those values or irrelevant for prediction of calories in food items.Therefore, we are dropping them from the analysis to avoid multicollinearity and improve model efficiency.
```{r}
# dropping the columns that is irrelevant or redundant for the analysis
cols_to_drop <- c(
  "Category",
  "Calories.from.Fat",
  "Saturated.Fat",
  "Trans.Fat",
  "Total.Fat....Daily.Value.",
  "Saturated.Fat....Daily.Value.",
  "Cholesterol....Daily.Value.",
  "Sodium....Daily.Value.",
  "Carbohydrates....Daily.Value.",
  "Sugars",
  "Dietary.Fiber....Daily.Value."
)
mcdonalds <- mcdonalds[, !(names(mcdonalds) %in% cols_to_drop)]
```



Now, we are converting the % Daily Value columns to absolute values. The daily values are based on a 2000 calorie diet, and the conversion is done using the following formula:
$$
\text{Absolute Value} = \left(\frac{\text{Percentage Daily Value}}{100}\right) \times \text{Daily Value}
$$
where the daily values are as follows:


```{r}

# Daily values (units must match dataset)
daily_values <- c(
  "Vitamin.A" = 900,    # mcg RAE
  "Vitamin.C" = 90,     # mg
  "Calcium" = 1300,     # mg
  "Iron" = 18           # mg
)

# Map of columns to their associated nutrients
conversion_map <- list(
  "Vitamin.A....Daily.Value." = "Vitamin.A",
  "Vitamin.C....Daily.Value." = "Vitamin.C",
  "Calcium....Daily.Value." = "Calcium",
  "Iron....Daily.Value." = "Iron"
)

# For each %DV column, calculate the absolute value and overwrite the same column with absolute value
for (dv_col in names(conversion_map)) {
  nutrient <- conversion_map[[dv_col]]
  
  if (dv_col %in% names(mcdonalds)) {
    # Create a new column name or overwrite the existing one
    new_col <- nutrient  # Replace the DV column with nutrient name only
    mcdonalds[[new_col]] <- (mcdonalds[[dv_col]] / 100) * daily_values[[nutrient]]
  }
}
```

The values in the serving size column contain both numbers and units (e.g., "15 oz"). In this step, we are extracting only the numeric part and discarding the unit, so "15 oz" becomes just 15.
```{r}
# Extract numeric part before "oz" or "fl oz" from the Serving Size column
mcdonalds$Serving.Size.Oz <- as.numeric(sub("([0-9.]+)\\s*(fl\\s*)?oz.*", "\\1", mcdonalds$Serving.Size))
head(mcdonalds)


# dropping the data points with null values that would be a hindrance for the analysis
mcdonalds <- na.omit(mcdonalds)

head(mcdonalds)

# Identify all columns to normalize (exclude Item and Serving Size Oz)
cols_to_normalize <- setdiff(names(mcdonalds), c("Item", "Serving.Size", "Serving.Size.Oz"))

# Convert values to per oz
mcdonalds[cols_to_normalize] <- lapply(mcdonalds[cols_to_normalize], function(col) col / mcdonalds$Serving.Size.Oz)

# dropping the remaining columns containing daily value percentages and serving size
cols_to_drop <- c(
  "Vitamin.A....Daily.Value.",
  "Vitamin.C....Daily.Value.",
  "Calcium....Daily.Value.",
  "Iron....Daily.Value.",
  "Serving.Size"
)
mcdonalds <- mcdonalds[, !(names(mcdonalds) %in% cols_to_drop)]


# saving all the column names except "Item" and "Serving Size Oz"
col_names <- setdiff(names(mcdonalds), c("Item","Serving.Size.Oz"))
```

```{r}
col_names
```
After the completion of the data preprocessing part, our data set has been narrowed down to one response variable $calories$ and ten explanatory variables:

1. x1 = $Total Fat$

2. x2 = $Cholestrol$

3. x3 = $Sodium$

4. x4 = $Carbohydrates$

5. x5 = $Dietary Fiber$

6. x6 = $Protein$

7. x7 = $Vitamin A$

8. x8 = $Vitamin C$ 

9. x9 = $Calcium$

10. x10 = $Iron$



```{r}

# creating new names: y for Calories, x1, x2, ... for the rest
new_names <- c("y", paste0("x", seq_along(col_names[-which(col_names == "Calories")])))

# creating a named vector to rename the columns
name_map <- setNames(new_names, c("Calories", col_names[col_names != "Calories"]))

# renaming the columns
names(mcdonalds)[names(mcdonalds) %in% names(name_map)] <- name_map[names(mcdonalds)[names(mcdonalds) %in% names(name_map)]]


# saving all the column names except "Item" and "Serving Size Oz"
col_names <- setdiff(names(mcdonalds), c("Item","Serving.Size.Oz"))
```



## Correlation Coefficients

Since we are looking at linear relationships between the outcome variable (calories) with each explanatory variable, it may be of interest to determine the correlation coefficients between the outcome variable with each explanatory variable. 
```{r}
corr_matrix <- cor(mcdonalds[,col_names], use="everything")
round(corr_matrix, 3)

corrplot.mixed(corr_matrix, lower.col = "black", number.cex = .7, upper = "ellipse")

```

<br>

## Multiple Regression

At this point we have interest in building a model for calories using some combination of the explanatory variables. Using multiple regression one initially build a model with all of the possible explanatory variables. Below is some R output for this Multiple Linear Regression (MLR) analysis. 

<br>

### General Form for a Multiple Regression Model 

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 +\dots + \beta_{14} X_{14}
$$

<br>
 
```{r}

model.1 <- lm(y~x1+x2+x3+x4+x5+x6+x7+x8+x9+x10, data=mcdonalds)

summary(model.1)

```
 
### Equation of the Model with all of the Explanatory Variables

(Note: This is referred to at the Full Model)

$$
Y = -0.120 + 9.066 X_1 - 0.014 X_2 + 0.002 X_3 + 4.031 X_4+ \dots + 0.922 X_{10}
$$
<br>

### Coefficient of Determination

<br>

Interpretation: **99.96** \% of the variability in calories is accounted for in this model, (i.e., is accounted for in the model between calories and the thirteen explanatory variables).

### Test for the Significance of the Model:

Ho:  None of the explanatory variables is a linear predictor of calories  (i.e., the model is not significant or is not useful in predicting the response)

<br>

Ha:  At least one of the explanatory variables is a significant linear predictor of calories  (i.e., the model is significant or at least some portion of the model is useful in predicting the response)


<br>

Test statistic: F* =  6.146e+04

<br>

P-value: < 2.2e-16

<br>

Conclusion: Reject Ho in favor of Ha. There is sufficient evidence to conclude that at least one of the explanatory variables is a significant linear predictor of calories  (i.e., the model is significant or at least some portion of the model is useful in predicting the response)

<br>


### Further Analysis

Since at least one of the independent variables is significant, we do further analysis to determine which one(s) is/are significant.

<br>

### Test for an Individual Predictor in this Model: 


Ho:  With x1, x2, x3, x4, x5, x6, x7, x8, and x10 in the model, x9 is not a linear predictor of y

Ha:  With x1, x2, x3, x4, x5, x6, x7, x8, and x10 in the model, x9 is a significant linear predictor of y
 

<br>

Test statistic: t* = -0.395

<br>

P-value: 0.69325  

<br>

Conclusion: Fail to reject Ho. There is insufficient evidence to conclude that with x1, x2, x3, x4, x5, x6, x7, x8, and x10 in the model, x9 is not a linear predictor of y

<br>


In the above model, x9 has the largest p-value and thus is the least significant. We could remove it from the model and rerun the analysis. Then we could test for significance of another independent variable. We could continue this process until only significant variables are left. This method for identifying the best model is referred to as **Backward Selection**.

Some selected output for the **Backward Selection** procedure:

```{r}

model.2 <- lm(y~x1+x2+x3+x4+x5+x6+x7+x8+x10, data=mcdonalds)

summary(model.2)

```
<br>


```{r}

model.3 <- lm(y~x1+x2+x3+x4+x5+x6+x7+x10, data=mcdonalds)

summary(model.3)

```
<br>


```{r}

model.4 <- lm(y~x1+x2+x3+x4+x5+x6+x10, data=mcdonalds)

summary(model.4)

```

```{r}

model.5 <- lm(y~x1+x2+x4+x5+x6+x10, data=mcdonalds)

summary(model.5)

```
### Final Model using Backward Selection: 

$$
Y = -0.116 + 9.112 X_1 - 0.015 X_2 + 4.015 X_4 - 0.642 X_5 + 3.822 X_6 + 1.166 X_{10}
$$

<br>



### Coefficient of Determination

<br>

(Assessing the fit of the model)  99.96 % of the variability in the calories is accounted for in this multiple linear regression model.

### Prediction: 

For a sample that has a total fat (x1) of 4, a cholesterol (x2) of 6, a carbohydrate (x4) of 8.2, a dietary fiber (x5) of 0.4, a protein (x6) of 3.4, and a iron (x10) of 0.54, we predict the calories (y)  to be  82.0.
 

<br>

### Estimation: 

For samples that have a total fat (x1) of 4, a cholesterol (x2) of 6, a carbohydrate (x4) of 8.2, a dietary fiber (x5) of 0.4, a protein (x6) of 3.4, and a iron (x10) of 0.54, we predict the calories (y)  to be  82.0. 

<br>

### Interpretation of Partial Slopes (B-weights or Coefficients):

For a fixed total fat (x1), a fixed cholesterol (x2), a fixed carbohydrates (x4), a fixed dietary fiber (x5), and a fixed protein (x6), as the iron increases by 1, the calories increases by 1.166.

For a fixed total fat (x1), a fixed cholesterol (x2), a fixed carbohydrates (x4), a fixed dietary fiber (x5), and a fixed iron (x10), as the protein increases by 1, the calories increases by 3.822.


For a fixed total fat (x1), a fixed cholesterol (x2), a fixed carbohydrates (x4), a fixed protein (x6), and a fixed iron (x10), as the dietary fiber increases by 1, the calories decreases by 0.642.


For a fixed total fat (x1), a fixed cholesterol (x2), a fixed dietary fiber (x5), a fixed protein (x6), and a fixed iron (x10), as the carbohydrates increases by 1, the calories increases by 4.015.


For a fixed total fat (x1), a fixed carbohydrates (x4), a fixed dietary fiber (x5), a fixed protein (x6), and a fixed iron (x10), as the cholesterol increases by 1, the calories decreases by 0.015.

For a fixed cholesterol (x2), a fixed carbohydrates (x4), a fixed dietary fiber (x5), a fixed protein (x6) and a fixed iron (x10), as the total fat increases by 1, the calories increases by 9.112.


<br>


### Confidence Intervals for the Coefficients

```{r}

confint(model.5, level=0.95)

```


We are 95% confident that for a fixed total fat (x1), a fixed cholesterol (x2), a fixed carbohydrates (x4), a fixed dietary fiber (x5), and a fixed protein (x6), as the iron increases by 1, the calories increases between 0.331 and 2.001.

We are 95% confident that for a fixed total fat (x1), a fixed cholesterol (x2), a fixed carbohydrates (x4), a fixed dietary fiber (x5), and a fixed iron (x10), as the protein increases by 1, the calories increases between 3.711  and 3.935.

We are 95% confident that for a fixed total fat (x1), a fixed cholesterol (x2), a fixed carbohydrates (x4), a fixed protein (x6), and a fixed iron (x10), as the dietary fiber increases by 1, the calories decreases between -1.145  and -0.140.

We are 95% confident that for a fixed total fat (x1), a fixed cholesterol (x2), a fixed dietary fiber (x5), a fixed protein (x6), and a fixed iron (x10), as the carbohydrates increases by 1, the calories increases between 3.975 and 4.056.

We are 95% confident that for a fixed total fat (x1), a fixed carbohydrates (x4), a fixed dietary fiber (x5), a fixed protein (x6), and a fixed iron (x10), as the cholesterol increases by 1, the calories decreases between -0.025 and -0.006.

We are 95% confident that for a fixed cholesterol (x2), a fixed carbohydrates (x4), a fixed dietary fiber (x5), a fixed protein (x6) and a fixed iron (x10), as the total fat increases by 1, the calories increases between 9.015 and 9.209.


<br>

### Residual Analysis 

This is checking the assumptions that need to be satisfied before it is appropriate to perform inference from a multiple regression model. 

1. The random errors are independent of each other.

2. The random errors are normally distributed

3. The random errors have constant variance (homoscedasticity)

(Note: There is a lot more to residual analysis than just these things, but to keep things focused on the inferential aspects of linear regression we will just do this quick check.)

<br>

```{r}

bptest(y~x1+x2+x4+x5+x6+x10, varformula = ~ fitted.values(model.5), studentize=TRUE, data=mcdonalds)
#oldpar <- par(oma=c(0,0,3,0), mfrow=c(2,2))
plot(model.5)
#par(oldpar)

```

Ho: The variability of the residuals is constant (homoscedasticity)

Ha: The variability of the residuals is not constant (heteroscedasticity)

Test statistic: $\chi^2_{BP}$ =  21.68

P-value: 3.222e-06

Conclusion (at the .05 level): Reject Ho in favor of Ha. There is sufficient evidence to conclude that the variability of the residuals is not constant (heteroscedasticity).


```{r}

# Examination of the distribution of the residuals

boxplot(model.5$residuals, col="lightblue", horizontal = TRUE)

shapiro.test(model.5$residuals)

```


Ho: The residuals follow a normal distribution

Ha: The residuals do not follow a normal distribution
 
Test statistic: W* =  0.94993

P-value: 1.066e-07

Conclusion (at the .05 level): Reject Ho in favor of Ha. There is sufficient evidence to conclude that the residuals do not follow a normal distribution.



In multiple linear regression, there are several assumption: the residuals (errors) should be normally distributed and should have constant variance. However, in our case, residuals do not follow a normal distribution and have non-constant variance. This suggests that our model may not be a good fit for the Macdonald's menu data.We could explore alternative regression techniques or include interaction terms in the model to better capture the relationships in the data.
